{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import parse\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout , TooManyRedirects\n",
    "import pandas as pd\n",
    "from flatten_json import flatten\n",
    "import pendulum \n",
    "from google.cloud import bigquery\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# %%\n",
    "# create logger\n",
    "LOGGER = logging.getLogger('log_building_permits')\n",
    "fhandler = logging.FileHandler(filename='log_building_permits.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "LOGGER.addHandler(fhandler)\n",
    "LOGGER.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def get_data_in_date_range(url, start_date, end_date):\n",
    "\n",
    "    if not url:\n",
    "        url = 'https://maps.victoria.ca/server/rest/services/OpenData/OpenData_PermitsAndLicences/MapServer/2/query'\n",
    "        LOGGER.info(f\"No url provided, using default: {url}\")\n",
    "    \n",
    "    if not start_date:\n",
    "        start_date = pendulum.now().subtract(days=7)\n",
    "        LOGGER.info(f\"No end date provided. Defaulting to last 7 days\")\n",
    "    else:\n",
    "        start_date = pendulum.parse(start_date)\n",
    "        \n",
    "        \n",
    "    if not end_date:\n",
    "        end_date = pendulum.now()\n",
    "        LOGGER.info(f\"No end date provided. Defaulting to latest date\")\n",
    "    else:\n",
    "        end_date = pendulum.parse(end_date)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    batches = pendulum.period(start_date, end_date).range('months')\n",
    "    \n",
    "    for start_year in batches:\n",
    "        end_year = start_year.add(months=1)\n",
    "    \n",
    "    # Prevent the end_date from going into the future\n",
    "        if end_year > end_date:\n",
    "            end_year = end_date\n",
    "    \n",
    "    \n",
    "        start_year_str = start_year.format('M-D-Y HH:MM:SS')\n",
    "        end_year_str = end_year.format('M-D-Y HH:MM:SS')\n",
    "        \n",
    "        params = {\n",
    "            'f': ['json'],\n",
    "            'outFields': ['*'],\n",
    "            'returnIdsOnly': ['false'],\n",
    "            'returnCountOnly': ['false'],\n",
    "            'returnGeometry': ['false'],\n",
    "            'spatialRel': ['esriSpatialRelIntersects'],\n",
    "            'where': [f\"(CREATED_DATE >= TIMESTAMP '{start_year_str}' AND CREATED_DATE <= TIMESTAMP '{end_year_str}')\"]\n",
    "            }\n",
    "        \n",
    "        response = requests.get(            \n",
    "        url=url, \n",
    "        params=params)\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('exceededTransferLimit') is not None:\n",
    "            LOGGER.info(f\"{start_year_str} to {end_year_str} exceeded transfer limits.\")\n",
    "            \n",
    "        \n",
    "        for row in data.get('features'):\n",
    "            yield flatten(row)\n",
    "\n",
    "def parse_data(data):\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df['attributes_created_date'] = df['attributes_created_date'].apply(lambda x: pendulum.from_format(str(x), \"x\").date())\n",
    "    df['attributes_completed_date'] = df['attributes_completed_date'].apply(lambda x: pendulum.from_format(str(x), \"x\").date())\n",
    "    \n",
    "    df_clean = df.assign(\n",
    "    attributes_objectid=df['attributes_objectid'].astype('int64'),\n",
    "    attributes_address=df['attributes_house'] + \" \" + df['attributes_street'],\n",
    "    attributes_work_value=df['attributes_bldgvalue'].astype('float32'),\n",
    "    attributes_x_long=df['attributes_x_long'].astype('float32'),\n",
    "    attributes_y_lat=df['attributes_y_lat'].astype('float32'),\n",
    "    _extracted_at=pendulum.now(\"UTC\")\n",
    "    \n",
    "    \n",
    "    )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def _get_field_schema(field):\n",
    "    name = field['name']\n",
    "    field_type = field.get('type', 'STRING')\n",
    "    mode = field.get('mode', 'NULLABLE')\n",
    "    fields = field.get('fields', [])\n",
    "\n",
    "    if fields:\n",
    "        subschema = []\n",
    "        for f in fields:\n",
    "            fields_res = _get_field_schema(f)\n",
    "            subschema.append(fields_res)\n",
    "    else:\n",
    "        subschema = []\n",
    "\n",
    "    field_schema = bigquery.SchemaField(name=name, \n",
    "        field_type=field_type,\n",
    "        mode=mode,\n",
    "        fields=subschema\n",
    "    )\n",
    "    return field_schema\n",
    "\n",
    "\n",
    "def parse_bq_json_schema(schema_filename):\n",
    "    schema = []\n",
    "    with open(schema_filename, 'r') as infile:\n",
    "        jsonschema = json.load(infile)\n",
    "\n",
    "    for field in jsonschema:\n",
    "        schema.append(_get_field_schema(field))\n",
    "\n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "project = os.getenv(\"PROJECT_ID\")\n",
    "dataset = \"yyj_raw\"\n",
    "table = \"building_permits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_in_date_range(url=None, start_date=\"1994-01-01\", end_date=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = parse_bq_json_schema(\"building_permit_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 90473 rows to data-stack-338105.yyj_raw.building_permits\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project=project)\n",
    "                \n",
    "# configure job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema = schema,\n",
    "    write_disposition='WRITE_APPEND'\n",
    "\n",
    ")\n",
    "\n",
    "# execute job \n",
    "load_job = client.load_table_from_dataframe(\n",
    "    df, '.'.join([project, dataset, table]), job_config = job_config\n",
    ")\n",
    "\n",
    "result = load_job.result()\n",
    "print(\"Written {} rows to {}\".format(result.output_rows, result.destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if (__name__ == '__main__'):\n",
    "    # this code block is used to parse commandline arguments into title and folder filters\n",
    "    if len(sys.argv) > 1:\n",
    "        valid = True\n",
    "        url = None\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "        schema_file = None\n",
    "        project = os.getenv(\"PROJECT_ID\")\n",
    "        dataset= \"yyj_raw\"\n",
    "        table=\"permit_data\"\n",
    "\n",
    "        for item in sys.argv[1:]:\n",
    "            if '--url' in item:\n",
    "                try:\n",
    "                    url = item.split('=')[1]\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Couldn't parse url\")\n",
    "\n",
    "            if '--start_date' in item:\n",
    "                try:\n",
    "                    start_date = item.split('=')[1]\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Couldn't parse start date. Example: --start_date=2021-01-01\")\n",
    "\n",
    "            if '--end_date' in item:\n",
    "                try:\n",
    "                    end_date = item.split('=')[1]\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Couldn't parse end date. Example: --end_date=2021-01-01\")\n",
    " \n",
    "            if '--schema_file' in item:\n",
    "                try:\n",
    "                    schema_file = item.split('=')[1]\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Couldn't parse schema file.\")\n",
    "                    valid = False\n",
    "                    break\n",
    "            if valid:\n",
    "                \n",
    "                data = get_data_in_date_range(url=url, start_date=start_date, end_date=end_date)\n",
    "                df = parse_data(data)\n",
    "                schema = parse_bq_json_schema(schema_file)\n",
    "                \n",
    "                client = bigquery.Client(project=project)\n",
    "                \n",
    "                # configure job\n",
    "                job_config = bigquery.LoadJobConfig(\n",
    "                    schema = schema,\n",
    "                    write_disposition='WRITE_APPEND'\n",
    "                \n",
    "                )\n",
    "                \n",
    "                # execute job \n",
    "                load_job = client.load_table_from_dataframe(\n",
    "                    df, '.'.join([project, dataset, table]), job_config = job_config\n",
    "                )\n",
    "                \n",
    "                result = load_job.result()\n",
    "                print(\"Written {} rows to {}\".format(result.output_rows, result.destination))\n",
    "    else:\n",
    "        LOGGER.info(\"NONE\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "802d60525c882086e05df490b51f4b17b4298a48e9ee71714228b870d17d546c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.dbt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
